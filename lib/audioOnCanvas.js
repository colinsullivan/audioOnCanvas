// Generated by CoffeeScript 1.3.1

/*
#  @file    audioOnCanvas.coffee
#
#  @author  Colin Sullivan <colinsul [at] gmail.com>
#
#           Copyright (c) 2012 Colin Sullivan
#           Licensed under the MIT license.
*/


(function() {
  var audioOnCanvas,
    __hasProp = {}.hasOwnProperty,
    __extends = function(child, parent) { for (var key in parent) { if (__hasProp.call(parent, key)) child[key] = parent[key]; } function ctor() { this.constructor = child; } ctor.prototype = parent.prototype; child.prototype = new ctor; child.__super__ = parent.prototype; return child; };

  audioOnCanvas = (typeof exports !== "undefined" && exports !== null) && this || (this.audioOnCanvas = {});

  /*
  #   @class  Base class for rendering an audio file on a canvas element.
  */


  audioOnCanvas.Renderer = (function() {

    Renderer.name = 'Renderer';

    /*
      # @constructor
      #
      # @param  canvasElement   The canvas element to draw the waveform on.
      # @param  buffer          The `AudioBuffer` instance to render.
    */


    function Renderer(options) {
      if (options == null) {
        options = {};
      }
      this.canvasElement = options.canvasElement;
      this.buffer = options.buffer;
      if (!(this.canvasElement != null)) {
        throw new Error("this.canvasElement is undefined");
      }
      if (!(this.buffer != null)) {
        throw new Error("this.buffer is undefined");
      }
      this.canvasCtx = this.canvasElement.getContext("2d");
      this.render();
    }

    /*
      # Should be overridden in subclasses to render audio plot.
    */


    Renderer.prototype.render = function() {
      return this;
    };

    return Renderer;

  })();

  /*
  #   @class    Used for generating a waveform inside a canvas
  #             element.
  #   @extends  audioOnCanvas.Renderer
  */


  audioOnCanvas.WaveformRenderer = (function(_super) {

    __extends(WaveformRenderer, _super);

    WaveformRenderer.name = 'WaveformRenderer';

    function WaveformRenderer() {
      return WaveformRenderer.__super__.constructor.apply(this, arguments);
    }

    WaveformRenderer.prototype.render = function() {
      var canvasHeight, canvasWidth, drawSample, midHeight, prevSamplePosition, samples, x, _i, _results,
        _this = this;
      WaveformRenderer.__super__.render.call(this);
      canvasHeight = this.canvasCtx.canvas.clientHeight;
      canvasWidth = this.canvasCtx.canvas.clientWidth;
      midHeight = canvasHeight / 2.0;
      prevSamplePosition = {
        x: 0,
        y: midHeight
      };
      samples = this.buffer.getChannelData(0);
      drawSample = function(x) {
        var newSamplePosition, sampleValue;
        sampleValue = samples[Math.floor((x / canvasWidth) * samples.length)];
        newSamplePosition = {
          x: x,
          y: midHeight - midHeight * sampleValue
        };
        _this.canvasCtx.beginPath();
        _this.canvasCtx.moveTo(prevSamplePosition.x, prevSamplePosition.y);
        _this.canvasCtx.lineTo(newSamplePosition.x, newSamplePosition.y);
        _this.canvasCtx.stroke();
        prevSamplePosition.x = newSamplePosition.x;
        return prevSamplePosition.y = newSamplePosition.y;
      };
      _results = [];
      for (x = _i = 0; 0 <= canvasWidth ? _i <= canvasWidth : _i >= canvasWidth; x = _i += 0.01) {
        _results.push(drawSample(x));
      }
      return _results;
    };

    return WaveformRenderer;

  })(audioOnCanvas.Renderer);

  /*
  #   @class  Used for generating a spectrum inside a canvas element.
  */


  audioOnCanvas.SpectrumRenderer = (function(_super) {

    __extends(SpectrumRenderer, _super);

    SpectrumRenderer.name = 'SpectrumRenderer';

    function SpectrumRenderer() {
      return SpectrumRenderer.__super__.constructor.apply(this, arguments);
    }

    SpectrumRenderer.prototype.render = function() {
      var canvasHeight, canvasWidth, finishRendering, frequencyData, frequencyDatum, playTime, prevSamplePosition, processNode, processSpectrumData, source, spectrumNode, temporaryAudioCtx,
        _this = this;
      SpectrumRenderer.__super__.render.call(this);
      canvasHeight = this.canvasCtx.canvas.clientHeight;
      canvasWidth = this.canvasCtx.canvas.clientWidth;
      temporaryAudioCtx = new webkitAudioContext();
      source = temporaryAudioCtx.createBufferSource();
      source.buffer = this.buffer;
      spectrumNode = temporaryAudioCtx.createAnalyser();
      spectrumNode.fftSize = 512;
      spectrumNode.smoothingTimeConstant = 0.5;
      source.connect(spectrumNode);
      processNode = temporaryAudioCtx.createJavaScriptNode(512, this.buffer.numberOfChannels, 1);
      frequencyDatum = [];
      playTime = null;
      finishRendering = function() {
        var freqBinWidth, j, spectrogram, timeChunk, x, _i, _results;
        console.log(frequencyDatum);
        processNode.disconnect(temporaryAudioCtx.destination);
        freqBinWidth = canvasWidth / frequencyDatum.length;
        spectrogram = context.createImageData(canvasWidth, canvasHeight);
        _results = [];
        for (x = _i = 0; 0 <= canvasWidth ? _i <= canvasWidth : _i >= canvasWidth; x = 0 <= canvasWidth ? ++_i : --_i) {
          timeChunk = frequencyDatum[Math.floor(x / canvasWidth)];
          _results.push((function() {
            var _j, _ref, _results1;
            _results1 = [];
            for (j = _j = 0, _ref = timeChunk.length; 0 <= _ref ? _j <= _ref : _j >= _ref; j = 0 <= _ref ? ++_j : --_j) {
              spectrogram.data[canvasWidth * j * 4 + x * 4] = 255;
              spectrogram.data[canvasWidth * j * 4 + x * 4 + 1] = 255;
              spectrogram.data[canvasWidth * j * 4 + x * 4 + 2] = 255;
              _results1.push(spectrogram.data[canvasWidth * j * 4 + x * 4 + 3] = 255 * timeChunk[j]);
            }
            return _results1;
          })());
        }
        return _results;
      };
      prevSamplePosition = {
        x: 0,
        y: canvasHeight
      };
      frequencyData = new Float32Array(512);
      processSpectrumData = function(e) {
        spectrumNode.getFloatFrequencyData(frequencyData);
        frequencyDatum.push({
          time: temporaryAudioCtx.currentTime,
          frequencyData: frequencyData
        });
        if (temporaryAudioCtx.currentTime + playTime > _this.buffer.length / _this.buffer.sampleRate) {
          return finishRendering();
        }
      };
      processNode.onaudioprocess = processSpectrumData;
      spectrumNode.connect(processNode);
      console.log("Processing");
      processNode.connect(temporaryAudioCtx.destination);
      playTime = temporaryAudioCtx.currentTime;
      return source.noteOn(0);
    };

    return SpectrumRenderer;

  })(audioOnCanvas.Renderer);

}).call(this);
